{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# https://www.kaggle.com/c/state-farm-distracted-driver-detection/data\n",
    "# download data and then create validation from train\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "path = 'data/distracted_driver/imgs/'\n",
    "\n",
    "if not os.path.isdir(path + 'val'): # make validation folder\n",
    "    print('Making new directory: {}'.format(path + 'val'))\n",
    "    os.mkdir(path + 'val')\n",
    "\n",
    "\n",
    "# assumes there's a folder called 'train' which has all the labeled dataset\n",
    "for driver_class in os.listdir(path + 'train/'): # move 20% of train to validation\n",
    "    images = os.listdir(path + 'train/' + driver_class)\n",
    "    val_list = random.sample(images, len(images) // 5)\n",
    "    \n",
    "    if not os.path.isdir(path + 'val/' + driver_class):\n",
    "        print('Making new directory: {}'.format(path + 'val/' + driver_class))\n",
    "        os.mkdir(path + 'val/' + driver_class)\n",
    "    for image in val_list:\n",
    "        os.rename(path + 'train/' + driver_class + '/' + image, path + 'val/' + driver_class + '/' + image)\n",
    "\n",
    "# assumes there's a folder called test and will put all unlabeled dataset in test/unlabeled\n",
    "if not os.path.isdir(path + 'test/unlabeled'): # moving test set into a folder of its own\n",
    "    os.rename(path + 'test/', path + 'unlabeled/')\n",
    "    os.mkdir(path + 'test/')\n",
    "    os.rename(path + 'unlabeled/', path + 'test/unlabeled/')\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from vgg16 import Vgg16\n",
    "import os\n",
    "\n",
    "path = 'data/distracted_driver/imgs/' # change for respective data set\n",
    "model_path = 'data/distracted_driver/models/' # change for respective data set\n",
    "batch_size = 64\n",
    "\n",
    "if not os.path.isdir(model_path): # make validation folder\n",
    "    print('Making new directory: {}'.format(model_path))\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "vgg = Vgg16() # imagenet weights already loaded\n",
    "# during training, shuffle should be set to true. Or else, it will likely receive training on the same class consecutively\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=True)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.finetune(train_batches) # optimizer is Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "17943/17943 [==============================] - 546s - loss: 2.0105 - acc: 0.4405 - val_loss: 0.5781 - val_acc: 0.8233\n"
     ]
    }
   ],
   "source": [
    "vgg.fit(train_batches, val_batches, nb_epoch=1)  # 2 minutes; 582 // 60, 582 % 60; copied from other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Epoch 1/1\n",
      "4481/4481 [==============================] - 545s - loss: 13.7040 - acc: 0.0674 - val_loss: 9.6724 - val_acc: 0.1034\n",
      "CPU times: user 12min 40s, sys: 2min, total: 14min 40s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vgg = Vgg16() # imagenet weights already loaded\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=True)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.finetune(train_batches) # optimizer is Adam\n",
    "\n",
    "val_batches.shuffle = True\n",
    "vgg.fit(val_batches, train_batches, nb_epoch=1) # using val as train set for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Epoch 1/1\n",
      "4481/4481 [==============================] - 545s - loss: 13.0972 - acc: 0.0864 - val_loss: 7.8026 - val_acc: 0.1222\n",
      "CPU times: user 12min 56s, sys: 1min 58s, total: 14min 54s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vgg = Vgg16() # imagenet weights already loaded\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=True)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.finetune(train_batches) # optimizer is Adam\n",
    "\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.fit(val_batches, train_batches, nb_epoch=1) # using val as train set for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Epoch 1/1\n",
      "17943/17943 [==============================] - 545s - loss: 1.9695 - acc: 0.4519 - val_loss: 0.5339 - val_acc: 0.8494\n",
      "CPU times: user 12min 38s, sys: 2min, total: 14min 38s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vgg = Vgg16() # imagenet weights already loaded\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=True)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.finetune(train_batches) # optimizer is Adam\n",
    "\n",
    "vgg.fit(train_batches, val_batches, nb_epoch=1) # using val as train set for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Epoch 1/1\n",
      "4481/4481 [==============================] - 546s - loss: 3.3954 - acc: 0.2002 - val_loss: 1.3783 - val_acc: 0.5483\n",
      "CPU times: user 12min 48s, sys: 2min, total: 14min 49s\n",
      "Wall time: 9min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vgg = Vgg16() # imagenet weights already loaded\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=True)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.finetune(train_batches) # optimizer is Adam\n",
    "\n",
    "\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=False)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=True)\n",
    "vgg.fit(val_batches, train_batches, nb_epoch=1) # using val as train set for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Epoch 1/1\n",
      "4481/4481 [==============================] - 545s - loss: 3.4472 - acc: 0.1915 - val_loss: 1.3256 - val_acc: 0.5706\n",
      "CPU times: user 12min 31s, sys: 1min 59s, total: 14min 31s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vgg = Vgg16() # imagenet weights already loaded\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=True)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.finetune(train_batches) # optimizer is Adam\n",
    "\n",
    "\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=False)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=True)\n",
    "vgg.fit(val_batches, train_batches, nb_epoch=1) # using val as train set for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 40s, sys: 23.7 s, total: 3min 4s\n",
      "Wall time: 1min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2507495528165582, 0.59428698951126979]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time vgg.model.evaluate_generator(val_batches, val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2619915684933269, 0.60008926578888644]\n",
      "CPU times: user 2min 37s, sys: 24.2 s, total: 3min 2s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_batches.shuffle = True\n",
    "print(vgg.model.evaluate_generator(val_batches, val_batches.nb_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 53s, sys: 1min 34s, total: 11min 27s\n",
      "Wall time: 7min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3255535448427311, 0.57064036114694339]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time vgg.model.evaluate_generator(train_batches, train_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Epoch 1/1\n",
      "4481/4481 [==============================] - 544s - loss: 3.0650 - acc: 0.2435 - val_loss: 1.3680 - val_acc: 0.5435\n",
      "CPU times: user 12min 44s, sys: 1min 57s, total: 14min 42s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vgg = Vgg16() # imagenet weights already loaded\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=True)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.finetune(train_batches) # optimizer is Adam\n",
    "\n",
    "\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=False)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size, shuffle=True)\n",
    "vgg.fit(val_batches, train_batches, nb_epoch=1) # using val as train set for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Epoch 1/1\n",
      "4481/4481 [==============================] - 219s - loss: 3.3646 - acc: 0.1991 - val_loss: 1.3506 - val_acc: 0.5499\n",
      "CPU times: user 5min 26s, sys: 48.5 s, total: 6min 15s\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vgg = Vgg16() # imagenet weights already loaded\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=True)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.finetune(train_batches) # optimizer is Adam\n",
    "\n",
    "\n",
    "train_batches = vgg.get_batches(path + 'val/', batch_size=batch_size, shuffle=True)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=True)\n",
    "vgg.fit(val_batches, train_batches, nb_epoch=1) # using val as train set for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# appears that training must be shuffled, but val can or cannot be shuffled\n",
    "# could be the finetune\n",
    "# start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%%time\n",
    "from vgg16 import Vgg16\n",
    "import os\n",
    "\n",
    "path = 'data/distracted_driver/imgs/' # change for respective data set\n",
    "model_path = 'data/distracted_driver/models/' # change for respective data set\n",
    "batch_size = 64\n",
    "\n",
    "vgg = Vgg16() # imagenet weights already loaded\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=True)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.finetune(train_batches) # optimizer is Adam\n",
    "\n",
    "\n",
    "train_batches = vgg.get_batches(path + 'val/', batch_size=batch_size, shuffle=False)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=True)\n",
    "print(vgg.fit(val_batches, train_batches, nb_epoch=1)) # using val as train set for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Epoch 1/1\n",
      "\r",
      " 128/4481 [..............................] - ETA: 177s - loss: 4.6485 - acc: 0.1094\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 256/4481 [>.............................] - ETA: 139s - loss: 4.3966 - acc: 0.1094\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 384/4481 [=>............................] - ETA: 125s - loss: 4.4045 - acc: 0.1198\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 512/4481 [==>...........................] - ETA: 119s - loss: 4.3512 - acc: 0.1230\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 640/4481 [===>..........................] - ETA: 111s - loss: 4.2928 - acc: 0.1203\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 768/4481 [====>.........................] - ETA: 106s - loss: 4.2789 - acc: 0.1185\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 896/4481 [====>.........................] - ETA: 100s - loss: 4.1741 - acc: 0.1272\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1024/4481 [=====>........................] - ETA: 96s - loss: 4.1369 - acc: 0.1289 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1152/4481 [======>.......................] - ETA: 91s - loss: 4.1314 - acc: 0.1250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1280/4481 [=======>......................] - ETA: 87s - loss: 4.1447 - acc: 0.1234\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1408/4481 [========>.....................] - ETA: 83s - loss: 4.1147 - acc: 0.1229\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1536/4481 [=========>....................] - ETA: 80s - loss: 4.0502 - acc: 0.1276\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1664/4481 [==========>...................] - ETA: 76s - loss: 3.9888 - acc: 0.1310\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1792/4481 [==========>...................] - ETA: 72s - loss: 3.9331 - acc: 0.1350\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1920/4481 [===========>..................] - ETA: 68s - loss: 3.8898 - acc: 0.1401\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "2048/4481 [============>.................] - ETA: 65s - loss: 3.8487 - acc: 0.1396\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "2176/4481 [=============>................] - ETA: 61s - loss: 3.8146 - acc: 0.1434\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "2304/4481 [==============>...............] - ETA: 58s - loss: 3.7783 - acc: 0.1484\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "2432/4481 [===============>..............] - ETA: 54s - loss: 3.7476 - acc: 0.1509\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "2560/4481 [================>.............] - ETA: 51s - loss: 3.7173 - acc: 0.1539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "2688/4481 [================>.............] - ETA: 47s - loss: 3.6874 - acc: 0.1566\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "2816/4481 [=================>............] - ETA: 44s - loss: 3.6484 - acc: 0.1619\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "2944/4481 [==================>...........] - ETA: 40s - loss: 3.6295 - acc: 0.1654\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3072/4481 [===================>..........] - ETA: 37s - loss: 3.6106 - acc: 0.1654\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3200/4481 [====================>.........] - ETA: 34s - loss: 3.5783 - acc: 0.1697\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3328/4481 [=====================>........] - ETA: 30s - loss: 3.5558 - acc: 0.1713\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3456/4481 [======================>.......] - ETA: 27s - loss: 3.5184 - acc: 0.1791\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3584/4481 [======================>.......] - ETA: 23s - loss: 3.4961 - acc: 0.1811\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3712/4481 [=======================>......] - ETA: 20s - loss: 3.4721 - acc: 0.1837\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3840/4481 [========================>.....] - ETA: 17s - loss: 3.4554 - acc: 0.1870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3968/4481 [=========================>....] - ETA: 13s - loss: 3.4264 - acc: 0.1888\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "4096/4481 [==========================>...] - ETA: 10s - loss: 3.3982 - acc: 0.1921\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "4224/4481 [===========================>..] - ETA: 6s - loss: 3.3650 - acc: 0.1958 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "4352/4481 [============================>.] - ETA: 3s - loss: 3.3419 - acc: 0.1992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "4480/4481 [============================>.] - ETA: 0s - loss: 3.3264 - acc: 0.2018\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "4481/4481 [==============================] - 237s - loss: 3.3259 - acc: 0.2020 - val_loss: 1.3247 - val_acc: 0.5360\n",
      "None\n",
      "CPU times: user 5min 34s, sys: 55.2 s, total: 6min 30s\n",
      "Wall time: 4min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3115550677868384, 0.53470207542959158]\n",
      "[1.3247410859733892, 0.53604106226288772]\n"
     ]
    }
   ],
   "source": [
    "print(vgg.model.evaluate_generator(val_batches, val_batches.nb_sample))\n",
    "print(vgg.model.evaluate_generator(train_batches, val_batches.nb_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#val_batches.shuffle = True\n",
    "vgg.fit(val_batches, train_batches, nb_epoch=1) # using val as train set for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training must be shuffled, validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 50s, sys: 25.3 s, total: 3min 15s\n",
      "Wall time: 1min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.8458090132832714, 0.11470653871903592]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time vgg.model.evaluate_generator(val_batches, val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4955237243782595, 0.12318678866324481]\n",
      "CPU times: user 2min 52s, sys: 24.9 s, total: 3min 17s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%time print(vgg.model.evaluate_generator(val_batches, val_batches.nb_sample))\n",
    "\"\"\"\n",
    "CPU times: user 2min 45s, sys: 27.9 s, total: 3min 13s\n",
    "Wall time: 2min 2s\n",
    "Out[4]:\n",
    "[0.53699896356502008, 0.84601651417094403]\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from vgg16 import Vgg16\n",
    "import os\n",
    "\n",
    "path = 'data/distracted_driver/imgs/' # change for respective data set\n",
    "model_path = 'data/distracted_driver/models/' # change for respective data set\n",
    "batch_size = 64\n",
    "\n",
    "if not os.path.isdir(model_path): # make validation folder\n",
    "    print('Making new directory: {}'.format(model_path))\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "vgg = Vgg16() # imagenet weights already loaded\n",
    "# during training, shuffle should be set to true. Or else, it will likely receive training on the same class consecutively\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=False)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.finetune(train_batches) # optimizer is Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((17943, 25088), (4481, 25088))\n",
      "CPU times: user 13min 17s, sys: 2min 13s, total: 15min 31s\n",
      "Wall time: 9min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import bcolz\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "def save_array(fname, arr): \n",
    "    c = bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "    \n",
    "def load_array(fname): \n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "def split_model(model, layer_type=None):\n",
    "    \"\"\"\n",
    "    Split model at first instance of layer_type.\n",
    "    Note: could not perform deepcopy of model\n",
    "    \"\"\"\n",
    "    if layer_type is None:\n",
    "        layer_type = Dense\n",
    "    first_dense_idx = [index for index, layer in enumerate(model.layers) if \n",
    "                       type(layer) is layer_type][0]    \n",
    "    return model.layers[:first_dense_idx], model.layers[first_dense_idx:]\n",
    "\n",
    "conv_layers, dense_layers = split_model(vgg.model)\n",
    "conv_model = Sequential(conv_layers)\n",
    "\n",
    "\n",
    "# this is possible since these features are much smaller than \n",
    "# raw images and are capable of being loaded into memory\n",
    "trn_features = conv_model.predict_generator(train_batches, val_samples=train_batches.nb_sample)\n",
    "val_features = conv_model.predict_generator(val_batches, val_samples=val_batches.nb_sample)\n",
    "\n",
    "save_array(model_path + 'train_conv_features_not_shuffled.bc/', trn_features)\n",
    "save_array(model_path + 'val_conv_features_not_shuffled.bc/', val_features)\n",
    "\n",
    "\n",
    "\n",
    "#trn_features = load_array(model_path + 'train_convlayer_features.bc/') # modified\n",
    "#val_features = load_array(model_path + 'valid_convlayer_features.bc/') # modified\n",
    "\n",
    "#trn_features = load_array(model_path + 'train_conv_features.bc/') # modified\n",
    "#val_features = load_array(model_path + 'val_conv_features.bc/') # modified\n",
    "\n",
    "#trn_features = load_array(model_path + 'train_conv_features_shuffled.bc/') # modified\n",
    "#val_features = load_array(model_path + 'val_conv_features_not_shuffled.bc/') # modified\n",
    "\n",
    "\n",
    "trn_features = load_array(model_path + 'train_conv_features_not_shuffled.bc/') # modified\n",
    "val_features = load_array(model_path + 'val_conv_features_not_shuffled.bc/') # modified\n",
    "\n",
    "\n",
    "print(trn_features.shape, val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cannot modify layers after they have been created\n",
    "\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def get_fc_model_2(conv_layers, dense_layers, opt=None, new_dropout_p=None):\n",
    "    \n",
    "    def proc_wgts(layer): # don't know how to reflate the weights if given\n",
    "        # different p for different layers\n",
    "        return [o / 2 for o in layer.get_weights()]\n",
    "    \n",
    "    def make_new_layer(layer, new_dropout_p=0.0):\n",
    "        if type(layer) is Dense:\n",
    "            return Dense(output_dim=layer.output_dim, input_shape=\n",
    "                         layer.input_shape[1:], activation=layer.activation)\n",
    "        # notice the input_shape is a slice, also takes care of number of \n",
    "        # output nodes and softmax\n",
    "        elif type(layer) is Dropout:\n",
    "            return Dropout(p=new_dropout_p, input_shape=layer.input_shape[1:])\n",
    "        else:\n",
    "            raise Exception('Unexpected layer')\n",
    "    \n",
    "    if new_dropout_p is None:\n",
    "        new_dropout_p = 0.0\n",
    "    model = Sequential([make_new_layer(layer, new_dropout_p) for layer in dense_layers])\n",
    "\n",
    "    for l1,l2 in zip(model.layers, dense_layers): \n",
    "        l1.set_weights(proc_wgts(l2))\n",
    "        \n",
    "    if opt is None:\n",
    "        opt = Adam(lr=0.00001) # need small learning rate or else \n",
    "        # loss will increase and accuracy will decrease\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def onehot(x): \n",
    "    return np.array(OneHotEncoder().fit_transform(x.reshape(-1, 1)).todense())\n",
    "\n",
    "\n",
    "trn_labels = onehot(train_batches.classes)\n",
    "val_labels = onehot(val_batches.classes)\n",
    "\n",
    "model = get_fc_model_2(conv_layers, dense_layers)\n",
    "#print(model.evaluate(trn_features, trn_labels))\n",
    "#print(model.evaluate(val_features, val_labels))\n",
    "\"\"\"\n",
    "17943/17943 [==============================] - 6s     \n",
    "[1.7024225849694168, 0.8673577439703285]\n",
    "4480/4481 [============================>.] - ETA: 0s[1.7144375196447119, 0.85538942200401691]\n",
    "\"\"\";\n",
    "# it appears that splitting the model up and evaluate will give slightly different results than\n",
    "# original VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17943 samples, validate on 4481 samples\n",
      "Epoch 1/2\n",
      "17943/17943 [==============================] - 27s - loss: 0.2791 - acc: 0.9308 - val_loss: 0.0329 - val_acc: 0.9909\n",
      "Epoch 2/2\n",
      "17943/17943 [==============================] - 28s - loss: 0.0067 - acc: 0.9988 - val_loss: 0.0204 - val_acc: 0.9951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11e0846810>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_features, trn_labels, nb_epoch=2, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))\n",
    "# model.optimizer.lr.get_value()\n",
    "# model.optimizer.lr.set_value()\n",
    "\n",
    "\n",
    "# appears without loading weights from 1 epoch of training, training with small learning rate still works well\n",
    "# looks like conv features have problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training shuffle, val not shuffled didn't work\n",
    "# however, for preprocessing, must not shuffle training because won't match with the labels. Fit has shuffle parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.fit_data() # vgg.fit_data(trn, labels, val, val_labels, nb_epoch=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n",
      "Epoch 1/1\n",
      "4480/4481 [============================>.] - ETA: 0s - loss: 13.3826 - acc: 0.0701"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7069849f9626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# optimizer is Adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/courses/deeplearning1/nbs/vgg16.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, batches, val_batches, nb_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m    212\u001b[0m         self.model.fit_generator(batches, samples_per_epoch=batches.nb_sample, nb_epoch=nb_epoch,\n\u001b[0;32m--> 213\u001b[0;31m                 validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1469\u001b[0m                         val_outs = self.evaluate_generator(validation_data,\n\u001b[1;32m   1470\u001b[0m                                                            \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m                                                            max_q_size=max_q_size)\n\u001b[0m\u001b[1;32m   1472\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m                         \u001b[0;31m# no need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1552\u001b[0m                                 'or (x, y). Found: ' + str(generator_output))\n\u001b[1;32m   1553\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                 \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from vgg16 import Vgg16\n",
    "import os\n",
    "\n",
    "path = 'data/distracted_driver/imgs/' # change for respective data set\n",
    "model_path = 'data/distracted_driver/models/' # change for respective data set\n",
    "batch_size = 64\n",
    "\n",
    "if not os.path.isdir(model_path): # make validation folder\n",
    "    print('Making new directory: {}'.format(model_path))\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "vgg = Vgg16() # imagenet weights already loaded\n",
    "# during training, shuffle should be set to true. Or else, it will likely receive training on the same class consecutively\n",
    "train_batches = vgg.get_batches(path + 'train/', batch_size=batch_size, shuffle=False)\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "vgg.finetune(train_batches) # optimizer is Adam\n",
    "\n",
    "vgg.fit(val_batches, train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tem4 = vgg.model.predict_generator(val_batches, val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4481/4481 [==============================] - 236s - loss: 5.4945 - acc: 0.2767 - val_loss: 8.0210 - val_acc: 0.2428\n"
     ]
    }
   ],
   "source": [
    "vgg.fit(val_batches, val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4481/4481 [==============================] - 118s - loss: 8.8082 - acc: 0.1544   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc334c71dd0>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.fit_generator(val_batches, val_batches.nb_sample, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.evaluate_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.fit_generator(val_batches, val_batches.nb_sample, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4481 images belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:57<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "val_batches = vgg.get_batches(path + 'val/', batch_size=batch_size * 2, shuffle=False)\n",
    "\n",
    "val_data = []\n",
    "for i in tqdm(range(int(np.ceil(val_batches.N / float(val_batches.batch_size))))):\n",
    "    val_data.append(next(val_batches)[0])\n",
    "\n",
    "val_data = np.concatenate(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4480/4481 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.082116212921133538, 0.98170051327828611]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_features, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cannot modify layers after they have been created\n",
    "\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def get_fc_model_2(conv_layers, dense_layers, opt=None, new_dropout_p=None):\n",
    "    \n",
    "    def proc_wgts(layer): # don't know how to reflate the weights if given\n",
    "        # different p for different layers\n",
    "        return [o / 2 for o in layer.get_weights()]\n",
    "    \n",
    "    def make_new_layer(layer, new_dropout_p=0.0):\n",
    "        if type(layer) is Dense:\n",
    "            return Dense(output_dim=layer.output_dim, input_shape=\n",
    "                         layer.input_shape[1:], activation=layer.activation)\n",
    "        # notice the input_shape is a slice, also takes care of number of \n",
    "        # output nodes and softmax\n",
    "        elif type(layer) is Dropout:\n",
    "            return Dropout(p=new_dropout_p, input_shape=layer.input_shape[1:])\n",
    "        else:\n",
    "            raise Exception('Unexpected layer')\n",
    "    \n",
    "    if new_dropout_p is None:\n",
    "        new_dropout_p = 0.0\n",
    "    model = Sequential([make_new_layer(layer, new_dropout_p) for layer in dense_layers])\n",
    "\n",
    "    for l1,l2 in zip(model.layers, dense_layers): \n",
    "        l1.set_weights(proc_wgts(l2))\n",
    "        \n",
    "    if opt is None:\n",
    "        opt = Adam(lr=0.00001) # need small learning rate or else \n",
    "        # loss will increase and accuracy will decrease\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def onehot(x): \n",
    "    return np.array(OneHotEncoder().fit_transform(x.reshape(-1, 1)).todense())\n",
    "\n",
    "\n",
    "trn_labels = onehot(train_batches.classes)\n",
    "val_labels = onehot(val_batches.classes)\n",
    "\n",
    "model = get_fc_model_2(conv_layers, dense_layers)\n",
    "print(model.evaluate(trn_features, trn_labels))\n",
    "print(model.evaluate(val_features, val_labels))\n",
    "\"\"\"\n",
    "17943/17943 [==============================] - 6s     \n",
    "[1.7024225849694168, 0.8673577439703285]\n",
    "4480/4481 [============================>.] - ETA: 0s[1.7144375196447119, 0.85538942200401691]\n",
    "\"\"\";\n",
    "# it appears that splitting the model up and evaluate will give slightly different results than\n",
    "# original VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr.set_value(float(model.optimizer.lr.get_value() * 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17943/17943 [==============================] - 6s     \n",
      "[2.283849432590261, 0.13944156495569301]\n",
      "4480/4481 [============================>.] - ETA: 0s[2.3030579957917343, 0.10667261771925909]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(trn_features, trn_labels))\n",
    "print(model.evaluate(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17943 samples, validate on 4481 samples\n",
      "Epoch 1/2\n",
      "17943/17943 [==============================] - 28s - loss: 2.3086 - acc: 0.1023 - val_loss: 2.3018 - val_acc: 0.0953\n",
      "Epoch 2/2\n",
      " 2176/17943 [==>...........................] - ETA: 24s - loss: 2.2890 - acc: 0.1314"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-614a80383f91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(trn_features, trn_labels, nb_epoch=2, \n\u001b[0;32m----> 2\u001b[0;31m              batch_size=batch_size, validation_data=(val_features, val_labels))\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.optimizer.lr.get_value()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# model.optimizer.lr.set_value()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    822\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(trn_features, trn_labels, nb_epoch=2, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))\n",
    "# model.optimizer.lr.get_value()\n",
    "# model.optimizer.lr.set_value()\n",
    "\n",
    "\n",
    "# appears without loading weights from 1 epoch of training, training with small learning rate still works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'no_dropout.h5')\n",
    "model.load_weights(model_path+'no_dropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17943 samples, validate on 4481 samples\n",
      "Epoch 1/2\n",
      "17943/17943 [==============================] - 28s - loss: 0.2772 - acc: 0.9322 - val_loss: 0.0205 - val_acc: 0.9951\n",
      "Epoch 2/2\n",
      "17943/17943 [==============================] - 28s - loss: 0.0075 - acc: 0.9986 - val_loss: 0.0137 - val_acc: 0.9955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe65cfd75d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_features, trn_labels, nb_epoch=2, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))\n",
    "# model.optimizer.lr.get_value()\n",
    "# model.optimizer.lr.set_value()\n",
    "\n",
    "\n",
    "# appears without loading weights from 1 epoch of training, training with small learning rate still works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17943 samples, validate on 4481 samples\n",
      "Epoch 1/2\n",
      "17943/17943 [==============================] - 28s - loss: 0.1390 - acc: 0.9711 - val_loss: 0.0292 - val_acc: 0.9922\n",
      "Epoch 2/2\n",
      "17943/17943 [==============================] - 28s - loss: 0.0054 - acc: 0.9989 - val_loss: 0.0143 - val_acc: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0cdfd06610>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_features, trn_labels, nb_epoch=2, \n",
    "             batch_size=batch_size, validation_data=(val_features, val_labels))\n",
    "\n",
    "#fc_model.fit(trn_features, trn_labels, nb_epoch=8, \n",
    "#             batch_size=batch_size, validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'no_dropout.h5')\n",
    "model.load_weights(model_path+'no_dropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## error: accuracy is low, essentially random. Could be features are bad or labels are shuffled, no: learning rate too high\n",
    "## error: splitting model to dense doesn't work: don't know why input shape is causing problem, yes: input shape incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inlinefrom sklearn.metrics import confusion_matrix\n",
    "from utils import get_batches, plot_confusion_matrix\n",
    "\n",
    "# from keras.optimizers import SGD, RMSprop\n",
    "# from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# submission\n",
    "import pandas as pd\n",
    "\n",
    "test_batches = vgg.get_batches(path + 'test/', batch_size=batch_size * 2, shuffle=False)\n",
    "df_filesnames = pd.DataFrame({'img': [name.split('/')[1] for name in test_batches.filenames]}) # have to give correct key name\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "predictions = pd.DataFrame(temp, columns=vgg.classes)\n",
    "#pd.read_csv('temp.csv', names=sorted(val_batches.class_indices, key=val_batches.class_indices.get))\n",
    "pd.concat([df_filesnames, predictions], axis=1).to_csv(path + 'submission_2.csv', index=False)\n",
    "\n",
    "FileLink(path + 'submission_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated\n",
    "Unless image augmentation, pre-calculating image features through conv-layers will speed up training in dense layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### predict_generator is faster than manual because it has queue for loading images, though sometimes gives memory error\n",
    "### try not to run any other scripts while this is performing\n",
    "test_batches = vgg.get_batches(path + 'test/', batch_size=batch_size * 2, shuffle=False)\n",
    "temp = vgg.model.predict_generator(test_batches, test_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "# manual training model is more robost than generator due to intermediate weight saves \n",
    "# 6:52\n",
    "from tqdm import tqdm\n",
    "import psutil; import os\n",
    "\n",
    "test_batches = vgg.get_batches(path + 'test/', batch_size=batch_size * 2, shuffle=False)\n",
    "\n",
    "images = []\n",
    "for i in tqdm(range(int(np.ceil(test_batches.N / float(test_batches.batch_size))))):\n",
    "    minibatch = next(test_batches)[0]\n",
    "    images.append(vgg.model.predict_on_batch(minibatch))\n",
    "#    print(psutil.Process(os.getpid()).memory_info().rss / 1e9)\n",
    "\n",
    "temp = np.concatenate(images)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/distracted_driver/imgs/submission_2.csv' target='_blank'>data/distracted_driver/imgs/submission_2.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/courses/deeplearning1/nbs/data/distracted_driver/imgs/submission_2.csv"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "predictions = pd.DataFrame(temp, columns=vgg.classes)\n",
    "#pd.read_csv('temp.csv', names=sorted(val_batches.class_indices, key=val_batches.class_indices.get))\n",
    "pd.concat([df_filesnames, predictions], axis=1).to_csv(path + 'submission_2.csv', index=False)\n",
    "\n",
    "FileLink(path + 'submission_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_batch, test_scores = vgg.test(path + 'test/', batch_size=batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4481 images belonging to 10 classes.\n",
      "CPU times: user 2min 48s, sys: 24.9 s, total: 3min 13s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%time val_batch, val_scores = vgg.test(path + 'val/', batch_size=batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_labels = onehot(val_batches.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 38s, sys: 24.7 s, total: 3min 3s\n",
      "Wall time: 1min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.2667276776813283, 0.090381611247489405]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#vgg.model.evaluate(val_data, val_labels)\n",
    "\n",
    "#vgg.model.evaluate_generator(get_batches(path + 'valid', gen, False, batch_size*2), val_batches.N)\n",
    "vgg.model.evaluate_generator(val_batches, val_batches.N)\n",
    "#vgg.model.evaluate(val_batch, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 51s, sys: 27.8 s, total: 3min 18s\n",
      "Wall time: 2min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.53699896356502008, 0.84601651417094403]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#vgg.model.evaluate(val_data, val_labels)\n",
    "\n",
    "#vgg.model.evaluate_generator(get_batches(path + 'valid', gen, False, batch_size*2), val_batches.N)\n",
    "vgg.model.evaluate_generator(val_batches, val_batches.N)\n",
    "#vgg.model.evaluate(val_batch, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.26672767305\n",
      "2.88110645346\n",
      "2.65701011186\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "print(log_loss(val_labels, val_scores))\n",
    "print(log_loss(val_labels, np.clip(val_scores, 0.025, 0.975)))\n",
    "print(log_loss(val_labels, np.clip(val_scores, 0.05, 0.95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in vgg.model.layers:\n",
    "    if type(layer) is Dense:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'distracted_driver/imgs/train/in/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c26404f728f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distracted_driver/imgs/train/in/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/courses/deeplearning1/nbs/utils.pyc\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(path, target_size)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/courses/deeplearning1/nbs/utils.pyc\u001b[0m in \u001b[0;36mget_batches\u001b[0;34m(dirname, gen, shuffle, batch_size, class_mode, target_size)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 target_size=(224,224)):\n\u001b[1;32m    100\u001b[0m     return gen.flow_from_directory(dirname, target_size=target_size,\n\u001b[0;32m--> 101\u001b[0;31m             class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mdim_ordering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_ordering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             save_to_dir=save_to_dir, save_prefix=save_prefix, save_format=save_format)\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, dim_ordering, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'distracted_driver/imgs/train/in/'"
     ]
    }
   ],
   "source": [
    "train_data = get_data('distracted_driver/imgs/train/in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use batch size of 1 since we're just doing preprocessing on the CPU\n",
    "val_batches = get_batches(path + 'valid', shuffle=False, batch_size=1)\n",
    "batches = get_batches(path + 'train', shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.clip; keras.metrics.categorical_crossentropy(val_labels, do_clip(val_preds, 0.93)).eval()\n",
    "# save data, weights\n",
    "# plot confusion\n",
    "# check most right, wrong, ambivalent\n",
    "# write about test, predict, fit, generator variants, and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "print('hello panda')\n",
    "print(5 + 3)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.categorical_crossentropy(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(val_classes, trn_classes, val_labels, trn_labels, \n",
    "#    val_filenames, filenames, test_filenames) = vgg.get_classes(path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
